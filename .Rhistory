theme(axis.text.x = element_text(angle = 45,
size=14,
color = "black")) +
geom_col() +
geom_text(
aes(label = n),
colour = "black",
fontface = "bold",
position=position_dodge(width=0.9),
vjust=-0.25)
p1
# Chunk 5
ep<-df %>%
filter(Sentiment == "Extremely Positive")
ep
# Chunk 6
df$TweetAt <- sub("-", "/", df$TweetAt)
df$TweetAt <- sub("-", "/", df$TweetAt)
df$TweetAt <- as.Date(df$TweetAt , format = "%d/%m/%y")
df = na.omit(df)
# Chunk 7
df$Tweets <- sapply(strsplit(as.character(df$OriginalTweet),split=' '),function(s) paste(s[!grepl('^@',s)],collapse=' '))
df$Tweets <- sapply(strsplit(as.character(df$Tweets),split=' '),function(s) paste(s[!grepl('^https:',s)],collapse=' '))
df$Tweets <- gsub("(?:\\s+|^)\\S*(?<!\\w)(?:@?|<filter>)(?!\\w)\\S*", "", df$Tweets, perl=TRUE)
# Chunk 8
# Word Counts:
text_wordcounts <- text_words  %>% count(word, sort = TRUE)
p3 <- text_wordcounts %>%
arrange(desc(n)) %>%
head(6) %>%
ggplot(aes(x=n,y=word))+
geom_col() + ylab("Location") +
theme(axis.text = element_text(size=12,
color = "black"),
legend.text = element_text(size=12))
head(text_wordcounts)
p3
# Chunk 9
docs <- Corpus(VectorSource(df$Tweets))
# pre-processing
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>% #Menghilang koma dll.
tm_map(stripWhitespace) %>%
tm_map(stemDocument) # menghilangkan kata mirip
docs <- tm_map(docs, content_transformer(tolower))
# menghilangkan konjungsi
docs <- tm_map(docs, removeWords, stopwords("english"))
# Chunk 10
# set.seed(1234) # for reproducibility
wordcloud1 <- wordcloud(docs
, max.words=500    # Set top n words
, random.order=FALSE # Words in decreasing freq
, rot.per=0.35       # % of vertical words
, use.r.layout=FALSE # Use C++ collision detection
, colors=brewer.pal(8, "Dark2"))
# Chunk 11
words <- df$Tweets
words <- data_frame(Text = words)
# head(words, n = 20)
words <- words %>%
# tm_map(removePunctuation) %>%
unnest_tokens(output = word, input = Text)%>%
anti_join(stop_words) %>% # Remove stop words in peter_words
count(word, sort = TRUE)
words = na.omit(words) #hapus baris yang kososng
# words <- gsub("[0-9]+", "", words$word)
# Chunk 12
p4 <- words %>%
filter(n > 2000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip() +
labs(x = "Word \n", y = "\n Count ", title = "Frequent Words\n") +
geom_text(aes(label = n), hjust = 1.2, colour = "white", fontface = "bold") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="darkblue", size = 12),
axis.title.y = element_text(face="bold", colour="darkblue", size = 12))
p4
# Chunk 13
ui <- shinyUI(
navbarPage("Analisis Sentimen",
tabPanel("Tabel",
mainPanel(
h2("Tabel Data Tweets Covid-19", align = "center"),
dataTableOutput('table'),width = "100%"
)
),
tabPanel("Sentiment",
mainPanel(
h2("Jumlah Tiap Sentiment", align = "center"),
plotOutput(outputId = "plot1"),width = "100%")
),
tabPanel("Tweets Covid-19 Perhari",
mainPanel(
h2("Jumlah Tweets dalam Satu Hari", align = "center"),
plotOutput(outputId = "plot2"),width = "100%")
),
tabPanel("Lokasi",
mainPanel(
h2("Jumlah Tweets Berdasarkan Lokasi",
align = "center"),
plotOutput(outputId = "plot3"),width = "100%")
),
tabPanel("Jumlah Kata",
mainPanel(
h2("Jumlah Kata Pada Tweets",
align = "center"),
plotOutput(outputId = "plot4"),width = "100%")
),
tabPanel("Word Cloud",
mainPanel(
h2("word Cloud Covid-19 Tweets",
align = "center"),
plotOutput(outputId = "wordcloud"),width = "100%")
)
)
)
server <- function(input, output) {
output$table <-renderDataTable(df) # tabel tweets
output$plot1 <-renderPlot({ # plot jumlah sentimen
plot(p1)
})
output$plot2 <-renderPlot({ # plot tweets jumlah harian
plot(p2)
})
output$plot3 <-renderPlot({ # plot lokasi tweets
plot(p3)
})
output$plot4 <-renderPlot({ # plot jumlah kata
plot(p4)
})
output$wordcloud <-renderPlot({  # plot word cloud
wordcloud(docs
, max.words=500    # Set top n words
, random.order=FALSE # Words in decreasing freq
, rot.per=0.35       # % of vertical words
, use.r.layout=FALSE # Use C++ collision detection
, colors=brewer.pal(8, "Dark2"))
})
}
# Chunk 14
shinyApp(ui, server) #tampilkan dalam shiny
# Chunk 1: setup
knitr::opts_chunk$set(echo = FALSE)
# Chunk 2
setwd(dirname(rstudioapi::getSourceEditorContext()$path)) # auto set directory
# library(readr)
library(tm) # pre-prosessing data
library(corpus) # fungsi corpus
library(tidymodels)
library(stringr)
library(dplyr) # Data wrangling & manipulation
library(tidytext) # For unnest_tokens
library(ggplot2) # For data visualizations & graphs
# wordcloud
library(wordcloud)
library(RColorBrewer)
# library shiny
library(shiny)
# Chunk 3
df <- read.csv("Corona_NLP_train.csv")
df
# Chunk 4
p1 <- df %>%
group_by(Sentiment) %>%
tally() %>%
ggplot(aes(x=Sentiment, y=n, fill = Sentiment)) +
theme(axis.text.x = element_text(angle = 45,
size=14,
color = "black")) +
geom_col() +
geom_text(
aes(label = n),
colour = "black",
fontface = "bold",
position=position_dodge(width=0.9),
vjust=-0.25)
p1
# Chunk 5
df$TweetAt <- sub("-", "/", df$TweetAt)
df$TweetAt <- sub("-", "/", df$TweetAt)
df$TweetAt <- as.Date(df$TweetAt , format = "%d/%m/%y")
df = na.omit(df)
# Chunk 6
df$Tweets <- sapply(strsplit(as.character(df$OriginalTweet),split=' '),function(s) paste(s[!grepl('^@',s)],collapse=' '))
df$Tweets <- sapply(strsplit(as.character(df$Tweets),split=' '),function(s) paste(s[!grepl('^https:',s)],collapse=' '))
df$Tweets <- gsub("(?:\\s+|^)\\S*(?<!\\w)(?:@?|<filter>)(?!\\w)\\S*", "", df$Tweets, perl=TRUE)
# Chunk 7
# Word Counts:
text_wordcounts <- text_words  %>% count(word, sort = TRUE)
p3 <- text_wordcounts %>%
arrange(desc(n)) %>%
head(6) %>%
ggplot(aes(x=n,y=word))+
geom_col() + ylab("Location") +
theme(axis.text = element_text(size=12,
color = "black"),
legend.text = element_text(size=12))
head(text_wordcounts)
p3
# Chunk 8
docs <- Corpus(VectorSource(df$Tweets))
# pre-processing
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>% #Menghilang koma dll.
tm_map(stripWhitespace) %>%
tm_map(stemDocument) # menghilangkan kata mirip
docs <- tm_map(docs, content_transformer(tolower))
# menghilangkan konjungsi
docs <- tm_map(docs, removeWords, stopwords("english"))
# Chunk 9
# set.seed(1234) # for reproducibility
wordcloud1 <- wordcloud(docs
, max.words=500    # Set top n words
, random.order=FALSE # Words in decreasing freq
, rot.per=0.35       # % of vertical words
, use.r.layout=FALSE # Use C++ collision detection
, colors=brewer.pal(8, "Dark2"))
# Chunk 10
words <- df$Tweets
words <- data_frame(Text = words)
# head(words, n = 20)
words <- words %>%
# tm_map(removePunctuation) %>%
unnest_tokens(output = word, input = Text)%>%
anti_join(stop_words) %>% # Remove stop words in peter_words
count(word, sort = TRUE)
words = na.omit(words) #hapus baris yang kososng
# words <- gsub("[0-9]+", "", words$word)
# Chunk 11
p4 <- words %>%
filter(n > 2000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip() +
labs(x = "Word \n", y = "\n Count ", title = "Frequent Words\n") +
geom_text(aes(label = n), hjust = 1.2, colour = "white", fontface = "bold") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="darkblue", size = 12),
axis.title.y = element_text(face="bold", colour="darkblue", size = 12))
p4
# Chunk 12
ui <- shinyUI(
navbarPage("Analisis Sentimen",
tabPanel("Tabel",
mainPanel(
h2("Tabel Data Tweets Covid-19", align = "center"),
dataTableOutput('table'),width = "100%"
)
),
tabPanel("Sentiment",
mainPanel(
h2("Jumlah Tiap Sentiment", align = "center"),
plotOutput(outputId = "plot1"),width = "100%")
),
tabPanel("Tweets Covid-19 Perhari",
mainPanel(
h2("Jumlah Tweets dalam Satu Hari", align = "center"),
plotOutput(outputId = "plot2"),width = "100%")
),
tabPanel("Lokasi",
mainPanel(
h2("Jumlah Tweets Berdasarkan Lokasi",
align = "center"),
plotOutput(outputId = "plot3"),width = "100%")
),
tabPanel("Jumlah Kata",
mainPanel(
h2("Jumlah Kata Pada Tweets",
align = "center"),
plotOutput(outputId = "plot4"),width = "100%")
),
tabPanel("Word Cloud",
mainPanel(
h2("word Cloud Covid-19 Tweets",
align = "center"),
plotOutput(outputId = "wordcloud"),width = "100%")
)
)
)
server <- function(input, output) {
output$table <-renderDataTable(df) # tabel tweets
output$plot1 <-renderPlot({ # plotsentimen
plot(p1)
})
output$plot2 <-renderPlot({ # plot tweets  harian
plot(p2)
})
output$plot3 <-renderPlot({ # plot lokasi tweets
plot(p3)
})
output$plot4 <-renderPlot({ # plot jumlah kata
plot(p4)
})
output$wordcloud <-renderPlot({  # plot word cloud
wordcloud(docs
, max.words=500    # Set top n words
, random.order=FALSE # Words in decreasing freq
, rot.per=0.35       # % of vertical words
, use.r.layout=FALSE # Use C++ collision detection
, colors=brewer.pal(8, "Dark2"))
})
}
# Chunk 13
shinyApp(ui, server) #tampilkan dalam shiny
# set.seed(1234) # for reproducibility
wordcloud1 <- wordcloud(docs
, max.words=500    # Set top n words
, random.order=FALSE # Words in decreasing freq
, rot.per=0.35       # % of vertical words
, use.r.layout=FALSE # Use C++ collision detection
, colors=brewer.pal(8, "Blue"))
# set.seed(1234) # for reproducibility
wordcloud1 <- wordcloud(docs
, max.words=500    # Set top n words
, random.order=FALSE # Words in decreasing freq
, rot.per=0.35       # % of vertical words
, use.r.layout=FALSE # Use C++ collision detection
, colors=brewer.pal(1, "Dark2"))
# Word Counts:
text_wordcounts <- text_words  %>% count(word, sort = TRUE)
p3 <- text_wordcounts %>%
arrange(desc(n)) %>%
head(6) %>%
ggplot(aes(x=n,y=word))+
geom_col() + ylab("Location") +
theme(axis.text = element_text(size=12,
color = "blue"),
legend.text = element_text(size=12))
head(text_wordcounts)
p3
# Word Counts:
text_wordcounts <- text_words  %>% count(word, sort = TRUE)
p3 <- text_wordcounts %>%
arrange(desc(n)) %>%
head(6) %>%
ggplot(aes(x=n,y=word))+
geom_col() + ylab("Location") +
theme(axis.text = element_text(size=12,
color = "black"),
legend.text = element_text(size=12))
head(text_wordcounts)
p3
# Chunk 1: setup
knitr::opts_chunk$set(echo = FALSE)
# Chunk 2
setwd(dirname(rstudioapi::getSourceEditorContext()$path)) # auto set directory
# library(readr)
library(tm) # pre-prosessing data
library(corpus) # fungsi corpus
library(tidymodels)
library(stringr)
library(dplyr) # Data wrangling & manipulation
library(tidytext) # For unnest_tokens
library(ggplot2) # For data visualizations & graphs
# wordcloud
library(wordcloud)
library(RColorBrewer)
# library shiny
library(shiny)
# Chunk 3
df <- read.csv("Corona_NLP_train.csv")
df
# Chunk 4
p1 <- df %>%
group_by(Sentiment) %>%
tally() %>%
ggplot(aes(x=Sentiment, y=n, fill = Sentiment)) +
theme(axis.text.x = element_text(angle = 45,
size=14,
color = "black")) +
geom_col() +
geom_text(
aes(label = n),
colour = "black",
fontface = "bold",
position=position_dodge(width=0.9),
vjust=-0.25)
p1
# Chunk 5
df$TweetAt <- sub("-", "/", df$TweetAt)
df$TweetAt <- sub("-", "/", df$TweetAt)
df$TweetAt <- as.Date(df$TweetAt , format = "%d/%m/%y")
df = na.omit(df)
# Chunk 6
df$Tweets <- sapply(strsplit(as.character(df$OriginalTweet),split=' '),function(s) paste(s[!grepl('^@',s)],collapse=' '))
df$Tweets <- sapply(strsplit(as.character(df$Tweets),split=' '),function(s) paste(s[!grepl('^https:',s)],collapse=' '))
df$Tweets <- gsub("(?:\\s+|^)\\S*(?<!\\w)(?:@?|<filter>)(?!\\w)\\S*", "", df$Tweets, perl=TRUE)
# Chunk 7
# Word Counts:
text_wordcounts <- text_words  %>% count(word, sort = TRUE)
p3 <- text_wordcounts %>%
arrange(desc(n)) %>%
head(6) %>%
ggplot(aes(x=n,y=word))+
geom_col() + ylab("Location") +
theme(axis.text = element_text(size=12,
color = "black"),
legend.text = element_text(size=12))
head(text_wordcounts)
p3
# Chunk 8
docs <- Corpus(VectorSource(df$Tweets))
# pre-processing
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>% #Menghilang koma dll.
tm_map(stripWhitespace) %>%
tm_map(stemDocument) # menghilangkan kata mirip
docs <- tm_map(docs, content_transformer(tolower))
# menghilangkan konjungsi
docs <- tm_map(docs, removeWords, stopwords("english"))
# Chunk 9
# set.seed(1234) # for reproducibility
wordcloud1 <- wordcloud(docs
, max.words=500    # Set top n words
, random.order=FALSE # Words in decreasing freq
, rot.per=0.35       # % of vertical words
, use.r.layout=FALSE # Use C++ collision detection
, colors=brewer.pal(1, "Dark2"))
# Chunk 10
words <- df$Tweets
words <- data_frame(Text = words)
# head(words, n = 20)
words <- words %>%
# tm_map(removePunctuation) %>%
unnest_tokens(output = word, input = Text)%>%
anti_join(stop_words) %>% # Remove stop words in peter_words
count(word, sort = TRUE)
words = na.omit(words) #hapus baris yang kososng
# words <- gsub("[0-9]+", "", words$word)
# Chunk 11
p4 <- words %>%
filter(n > 2000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
coord_flip() +
labs(x = "Word \n", y = "\n Count ", title = "Frequent Words\n") +
geom_text(aes(label = n), hjust = 1.2, colour = "white", fontface = "bold") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="darkblue", size = 12),
axis.title.y = element_text(face="bold", colour="darkblue", size = 12))
p4
# Chunk 12
ui <- shinyUI(
navbarPage("Analisis Sentimen",
tabPanel("Tabel",
mainPanel(
h2("Tabel Data Tweets Covid-19", align = "center"),
dataTableOutput('table'),width = "100%"
)
),
tabPanel("Sentiment",
mainPanel(
h2("Jumlah Tiap Sentiment", align = "center"),
plotOutput(outputId = "plot1"),width = "100%")
),
tabPanel("Tweets Covid-19 Perhari",
mainPanel(
h2("Jumlah Tweets dalam Satu Hari", align = "center"),
plotOutput(outputId = "plot2"),width = "100%")
),
tabPanel("Lokasi",
mainPanel(
h2("Jumlah Tweets Berdasarkan Lokasi",
align = "center"),
plotOutput(outputId = "plot3"),width = "100%")
),
tabPanel("Jumlah Kata",
mainPanel(
h2("Jumlah Kata Pada Tweets",
align = "center"),
plotOutput(outputId = "plot4"),width = "100%")
),
tabPanel("Word Cloud",
mainPanel(
h2("WordCloud Covid-19 Tweets",
align = "center"),
plotOutput(outputId = "wordcloud"),width = "100%")
)
)
)
server <- function(input, output) {
output$table <-renderDataTable(df) # tabel tweets
output$plot1 <-renderPlot({ # plotsentimen
plot(p1)
})
output$plot2 <-renderPlot({ # plot tweets  harian
plot(p2)
})
output$plot3 <-renderPlot({ # plot lokasi tweets
plot(p3)
})
output$plot4 <-renderPlot({ # plot jumlah kata
plot(p4)
})
output$wordcloud <-renderPlot({  # plot word cloud
wordcloud(docs
, max.words=500    # Set top n words
, random.order=FALSE # Words in decreasing freq
, rot.per=0.35       # % of vertical words
, use.r.layout=FALSE # Use C++ collision detection
, colors=brewer.pal(1, "Dark2"))
})
}
# Chunk 13
shinyApp(ui, server) #tampilkan dalam shiny
